{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install py-enigma\n",
        "!pip install torchtext==0.10.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nj_qcI33w7yZ",
        "outputId": "56b38897-8513-44b0-a20b-c37055b991d2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: py-enigma in /usr/local/lib/python3.8/dist-packages (0.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchtext==0.10.0 in /usr/local/lib/python3.8/dist-packages (0.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchtext==0.10.0) (1.21.6)\n",
            "Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.8/dist-packages (from torchtext==0.10.0) (1.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchtext==0.10.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from torchtext==0.10.0) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.9.0->torchtext==0.10.0) (4.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.10.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.10.0) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.10.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.10.0) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8jJmPJ7RUFX8"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torchtext.legacy\n",
        "import random\n",
        "import re\n",
        "import copy\n",
        "import time\n",
        "import spacy\n",
        "import numpy as np\n",
        "from torch import nn, Tensor\n",
        "import matplotlib.pyplot as plt\n",
        "from torchtext.legacy.data import Field\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from torch.nn import Transformer\n",
        "from random import randrange, choices\n",
        "from enigma.machine import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generateSample(rotor = False, ring = False, reflector = False, plug = False, start = False, plainText = 'random'):\n",
        "  rotorSet = [\"I\", \"II\", \"III\", \"IV\", \"V\"]\n",
        "  reflectorSet = ['B', 'C']\n",
        "  alphabet = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M'\n",
        "      , 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
        "\n",
        "  rotorSettings = ''\n",
        "  ringSettings = ''\n",
        "  reflectorSettings = ''\n",
        "  plugSettings = ''\n",
        "  startSettings = ''\n",
        "\n",
        "  # Get rotor settings\n",
        "  if(rotor):\n",
        "    rotorSettings = ' '.join(random.sample(rotorSet, 3))\n",
        "  else:\n",
        "    rotorSettings = 'I II III'\n",
        "\n",
        "  # Get ring settings\n",
        "  if(ring):\n",
        "    ringSettings = ' '.join([str(randrange(26) + 1), str(randrange(26) + 1), str(randrange(26) + 1)])\n",
        "  else:\n",
        "    ringSettings = '1 1 1'\n",
        "\n",
        "  # Get reflector settings\n",
        "  if(reflector):\n",
        "    reflectorSettings = random.sample(reflectorSet, 1)[0]\n",
        "  else:\n",
        "    reflectorSettings = 'B'\n",
        "\n",
        "  # Get plugboard Settings\n",
        "  if(plug):\n",
        "    plugSet = alphabet\n",
        "    for x in range(10):\n",
        "      plugSettings += (plugSet.pop(randrange(len(plugSet))) + plugSet.pop(randrange(len(plugSet))) + ' ')\n",
        "    plugSettings = plugSettings[:-1]\n",
        "  else:\n",
        "    plugSettings = 'AB CD EF GH IJ KL MN OP QR ST'\n",
        "\n",
        "  # Get rotor starting positions\n",
        "  if(start):\n",
        "    startSettings = ''.join(random.sample(alphabet, 3))\n",
        "  else:\n",
        "    startSettings = ('AAA')\n",
        "\n",
        "  # Generate random text or use provided text\n",
        "  if(plainText == 'random'):\n",
        "    plainText = ''.join(random.choices(alphabet, k=250))\n",
        "  plainText = re.sub(r'[^a-zA-Z]', '', plainText).upper()\n",
        "\n",
        "  # Create machine\n",
        "  machine = EnigmaMachine.from_key_sheet(\n",
        "                  rotors=rotorSettings,\n",
        "                  reflector=reflectorSettings,\n",
        "                  ring_settings=ringSettings,\n",
        "                  plugboard_settings=plugSettings)\n",
        "  # Set starting position\n",
        "  machine.set_display(startSettings)\n",
        "  cipherText = machine.process_text(plainText, replace_char=None)\n",
        "\n",
        "  targetParts = (rotorSettings, ringSettings, startSettings, reflectorSettings, plugSettings, plainText)\n",
        "\n",
        "  target = '-'.join(targetParts)\n",
        "  target = target + ('_' * (310 - len(target)))\n",
        "  sample = cipherText + ('_' * (310 - len(cipherText)))\n",
        "\n",
        "  return sample, target"
      ],
      "metadata": {
        "id": "Lqqo47pZwqho"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample, target = generateSample(rotor=True, ring=True, start=True, reflector=True, plug=True)\n",
        "print(len(sample))\n",
        "print(len(target))\n",
        "sample, target = generateSample(rotor=True, ring=True, start=True, reflector=True, plug=True)\n",
        "print(sample)\n",
        "print(target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fou9lc815BcM",
        "outputId": "89e52203-9c18-4dc4-d1b3-29a75e666090"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "310\n",
            "310\n",
            "EORXUNYQPISSPMRAUZEMFEDYKSZYRWBLECGXCKKXXMSUYCUYQOYTRDVDHESGRYALUHXWBCCTORRJBGZSHDXWRJPPJGLWHFUBZYAFKFAWHORSNQQGJLUWIMNQTRGSPBQKZRWOWOZFECZJIGKUUIOHERWAQRAMPBLTOBDZYJVIKTLJEQZYNUJMBDJPEDQSVFZRILNSOCPSQWKGDTMZZFOAVFVSCGRLJFSCGOENSUDBCCOYJUXYSPZHKKPLVI____________________________________________________________\n",
            "IV V III-18 3 16-FXO-B-IS JL MN WD HA RP CU KB TE GV-YFFZOQXYYYXYQOQQZQFFZQFFXOYFOZQOFXZZYOOOZXQYOOYFYZZYXQZZQYOXFFYOXYFQZYXYFOQZOOFQYFZQFYFXZZXXQXFZQXXYZZZQZXFOXXXQZXQZQQFXZQQOQQOZXXXFOQYQZQFOFYZYQXQXYFOQZYXFFQXOFZOFZXQOQOZXFZQOXQOZOQXYXFFYXXFQQZFZQXQOXYXFXXZXFOFQFYFOFZZOYZYZXZQXZYOYQFYQOYZQFOYXOZOQOO_______\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedder(nn.Module):\n",
        "  def __init__(self, vocab_size, d_model):\n",
        "    super().__init__()\n",
        "    self.embed = nn.Embedding(vocab_size, d_model)\n",
        "  def forward(self, x):\n",
        "    return self.embed(x)\n",
        "        \n",
        "class PositionalEncoder(nn.Module):\n",
        "  def __init__(self, d_model, max_seq_len = 310):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    \n",
        "    # create constant 'pe' matrix with values dependant on \n",
        "    # pos and i\n",
        "    pe = torch.zeros(max_seq_len, d_model)\n",
        "    for pos in range(max_seq_len):\n",
        "      for i in range(0, d_model, 2):\n",
        "        pe[pos, i] = \\\n",
        "        math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
        "        pe[pos, i + 1] = \\\n",
        "        math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
        "          \n",
        "    pe = pe.unsqueeze(0)\n",
        "    self.register_buffer('pe', pe)\n",
        "\n",
        "  \n",
        "  def forward(self, x):\n",
        "    # make embeddings relatively larger\n",
        "    x = x * math.sqrt(self.d_model)\n",
        "    #add constant to embedding\n",
        "    seq_len = x.size(1)\n",
        "    x = x + self.pe[:,:seq_len]#.cuda()\n",
        "    return x\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, heads, d_model, dropout = 0.1):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.d_model = d_model\n",
        "    self.d_k = d_model // heads\n",
        "    self.h = heads\n",
        "    \n",
        "    self.q_linear = nn.Linear(d_model, d_model)\n",
        "    self.v_linear = nn.Linear(d_model, d_model)\n",
        "    self.k_linear = nn.Linear(d_model, d_model)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.out = nn.Linear(d_model, d_model)\n",
        "  \n",
        "  def forward(self, q, k, v, mask=None):\n",
        "    bs = q.size(0)\n",
        "    \n",
        "    # perform linear operation and split into h heads\n",
        "    \n",
        "    k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
        "    q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
        "    v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
        "    \n",
        "    # transpose to get dimensions bs * h * sl * d_model\n",
        "    \n",
        "    k = k.transpose(1,2)\n",
        "    q = q.transpose(1,2)\n",
        "    v = v.transpose(1,2)# calculate attention using function we will define next\n",
        "    scores = attention(q, k, v, self.d_k, mask, self.dropout)\n",
        "    \n",
        "    # concatenate heads and put through final linear layer\n",
        "    concat = scores.transpose(1,2).contiguous()\\\n",
        "    .view(bs, -1, self.d_model)\n",
        "    \n",
        "    output = self.out(concat)\n",
        "\n",
        "    return output\n",
        "\n",
        "def attention(q, k, v, d_k, mask=None, dropout=None):\n",
        "  scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
        "\n",
        "  if mask is not None:\n",
        "    mask = mask.unsqueeze(1)\n",
        "    scores = scores.masked_fill(mask == 0, -1e9)\n",
        "  scores = F.softmax(scores, dim=-1)\n",
        "  \n",
        "  if dropout is not None:\n",
        "    scores = dropout(scores)\n",
        "      \n",
        "  output = torch.matmul(scores, v)\n",
        "  return output\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, d_model, d_ff=2048, dropout = 0.1):\n",
        "    super().__init__() \n",
        "    # We set d_ff as a default to 2048\n",
        "    self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "  def forward(self, x):\n",
        "    x = self.dropout(F.relu(self.linear_1(x)))\n",
        "    x = self.linear_2(x)\n",
        "    return x\n",
        "\n",
        "class Norm(nn.Module):\n",
        "  def __init__(self, d_model, eps = 1e-6):\n",
        "    super().__init__()\n",
        "\n",
        "    self.size = d_model\n",
        "    # create two learnable parameters to calibrate normalisation\n",
        "    self.alpha = nn.Parameter(torch.ones(self.size))\n",
        "    self.bias = nn.Parameter(torch.zeros(self.size))\n",
        "    self.eps = eps\n",
        "  def forward(self, x):\n",
        "    norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n",
        "    / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
        "    return norm\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self, d_model, heads, dropout = 0.1):\n",
        "    super().__init__()\n",
        "    self.norm_1 = Norm(d_model)\n",
        "    self.norm_2 = Norm(d_model)\n",
        "    self.attn = MultiHeadAttention(heads, d_model)\n",
        "    self.ff = FeedForward(d_model)\n",
        "    self.dropout_1 = nn.Dropout(dropout)\n",
        "    self.dropout_2 = nn.Dropout(dropout)\n",
        "      \n",
        "  def forward(self, x, mask):\n",
        "    x2 = self.norm_1(x)\n",
        "    x = x + self.dropout_1(self.attn(x2,x2,x2,mask))\n",
        "    x2 = self.norm_2(x)\n",
        "    x = x + self.dropout_2(self.ff(x2))\n",
        "    return x\n",
        "    \n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self, d_model, heads, dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.norm_1 = Norm(d_model)\n",
        "    self.norm_2 = Norm(d_model)\n",
        "    self.norm_3 = Norm(d_model)\n",
        "    \n",
        "    self.dropout_1 = nn.Dropout(dropout)\n",
        "    self.dropout_2 = nn.Dropout(dropout)\n",
        "    self.dropout_3 = nn.Dropout(dropout)\n",
        "    \n",
        "    self.attn_1 = MultiHeadAttention(heads, d_model)\n",
        "    self.attn_2 = MultiHeadAttention(heads, d_model)\n",
        "    self.ff = FeedForward(d_model)#.cuda()\n",
        "  def forward(self, x, e_outputs, src_mask, trg_mask):\n",
        "    x2 = self.norm_1(x)\n",
        "    x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))\n",
        "    x2 = self.norm_2(x)\n",
        "    x = x + self.dropout_2(self.attn_2(x2, e_outputs, e_outputs,\n",
        "    src_mask))\n",
        "    x2 = self.norm_3(x)\n",
        "    x = x + self.dropout_3(self.ff(x2))\n",
        "    return x\n",
        "def get_clones(module, N):\n",
        "  return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, vocab_size, d_model, N, heads):\n",
        "    super().__init__()\n",
        "    self.N = N\n",
        "    self.embed = Embedder(vocab_size, d_model)\n",
        "    self.pe = PositionalEncoder(d_model)\n",
        "    self.layers = get_clones(EncoderLayer(d_model, heads), N)\n",
        "    self.norm = Norm(d_model)\n",
        "  def forward(self, src, mask):\n",
        "    x = self.embed(src)\n",
        "    x = self.pe(x)\n",
        "    for i in range(self.N):\n",
        "      x = self.layers[i](x, mask)\n",
        "    return self.norm(x)\n",
        "    \n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, vocab_size, d_model, N, heads):\n",
        "    super().__init__()\n",
        "    self.N = N\n",
        "    self.embed = Embedder(vocab_size, d_model)\n",
        "    self.pe = PositionalEncoder(d_model)\n",
        "    self.layers = get_clones(DecoderLayer(d_model, heads), N)\n",
        "    self.norm = Norm(d_model)\n",
        "  def forward(self, trg, e_outputs, src_mask, trg_mask):\n",
        "    x = self.embed(trg)\n",
        "    x = self.pe(x)\n",
        "    for i in range(self.N):\n",
        "      x = self.layers[i](x, e_outputs, src_mask, trg_mask)\n",
        "    return self.norm(x)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self, src_vocab, trg_vocab, d_model, N, heads):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(src_vocab, d_model, N, heads)\n",
        "    self.decoder = Decoder(trg_vocab, d_model, N, heads)\n",
        "    self.out = nn.Linear(d_model, trg_vocab)\n",
        "  def forward(self, src, trg, src_mask, trg_mask):\n",
        "    e_outputs = self.encoder(src, src_mask)\n",
        "    d_output = self.decoder(trg, e_outputs, src_mask, trg_mask)\n",
        "    output = self.out(d_output)\n",
        "    return output"
      ],
      "metadata": {
        "id": "smfV5FiqUcpZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def createMasks(sample, target):\n",
        "  sampleLength = len(sample)\n",
        "  sampleMask = np.ones((1, sampleLength, sampleLength))\n",
        "  sampleMask = torch.from_numpy(sampleMask)\n",
        "\n",
        "  targetLength = len(target)\n",
        "  targetMask = np.tril(np.ones((1, targetLength, targetLength)), k=0).astype('uint8')\n",
        "  targetMask = torch.from_numpy(targetMask)\n",
        "\n",
        "  return sampleMask, targetMask"
      ],
      "metadata": {
        "id": "X7gg49ejQcwq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class tokenize():\n",
        "  sampleAlphabet = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '_']\n",
        "  targetAlphabet = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', ' ', '-', '_']\n",
        "  def __init__(self):\n",
        "    en = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    def tokenize_sample(input):\n",
        "      return [tok.text for tok in tokenize_sample(input)]\n",
        "\n",
        "    def tokenize_target(input):\n",
        "      return [tok.text for tok in tokenize_target(input)]\n",
        "\n",
        "    self.sampleText = Field(tokenize=tokenize_sample)\n",
        "    self.sampleText.build_vocab(self.sampleAlphabet)\n",
        "    self.targetText = Field(tokenize=tokenize_target, init_token=\"<sos>\", eos_token=\"<eos>\")\n",
        "    self.targetText.build_vocab(self.targetAlphabet)\n",
        "  def makeTokens(self, sample, target):\n",
        "    sampleOutput = []\n",
        "\n",
        "    while(len(sample) > 0):\n",
        "      sampleOutput.append(self.sampleText.vocab.stoi[sample[0]])\n",
        "      sample = sample[1:]\n",
        "\n",
        "    targetOutput = []\n",
        "\n",
        "    while(len(target) > 0):\n",
        "      targetOutput.append(self.targetText.vocab.stoi[target[0]])\n",
        "      target = target[1:]\n",
        "\n",
        "    return torch.tensor(sampleOutput, dtype=torch.int), torch.tensor(targetOutput, dtype=torch.int)\n"
      ],
      "metadata": {
        "id": "yYHjdxPOyri9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generateBatch(batchSize):\n",
        "  i = 0\n",
        "  iterations = []\n",
        "  tkn = tokenize()\n",
        "  while i  < batchSize:\n",
        "    i += 1\n",
        "    sample, target = generateSample(rotor=True, ring=True, reflector=False, start=False, plug=False)\n",
        "    sample, target = tkn.makeTokens(sample, target)  \n",
        "\n",
        "    sampleMask, targetMask = createMasks(sample, target)\n",
        "    iterations.append([sample, target, sampleMask, targetMask])\n",
        "  return iterations"
      ],
      "metadata": {
        "id": "jmjZLIVBVRvx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 512\n",
        "heads = 32\n",
        "N = 24\n",
        "\n",
        "batch_size = 100\n",
        "training_iters = 5000\n",
        "\n",
        "src_vocab = 50\n",
        "trg_vocab = 50\n",
        "\n",
        "model = Transformer(src_vocab, trg_vocab, d_model, N, heads)\n",
        "\n",
        "for p in model.parameters():\n",
        "  if p.dim() > 1:\n",
        "    nn.init.xavier_uniform_(p)\n",
        "i = 0\n",
        "losses = []\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "model.train()\n",
        "while i < training_iters:\n",
        "  batch = generateBatch(batch_size)\n",
        "  j = 0\n",
        "  for sample in batch:\n",
        "    preds = model(batch[j][0], batch[j][1], batch[j][2], batch[j][3])\n",
        "\n",
        "    loss = F.cross_entropy(preds.view(-1, preds.size(-1)), batch[j][1].type(torch.LongTensor))\n",
        "    optimizer.zero_grad()         \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    losses.append(loss.item())\n",
        "    j += 1\n",
        "  np.save('./losses', losses)\n",
        "  torch.save(model.state_dict(), './model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "0pND-Y7CBoPT",
        "outputId": "cf3a2eec-50bb-4c82-8b76-b88f4a8cbde2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-4ec268957b7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-84f09d35dbf9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trg, src_mask, trg_mask)\u001b[0m\n\u001b[1;32m    193\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0me_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m     \u001b[0md_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-84f09d35dbf9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, trg, e_outputs, src_mask, trg_mask)\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-84f09d35dbf9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, e_outputs, src_mask, trg_mask)\u001b[0m\n\u001b[1;32m    145\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m     \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     x = x + self.dropout_2(self.attn_2(x2, e_outputs, e_outputs,\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-84f09d35dbf9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, q, k, v, mask)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# calculate attention using function we will define next\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m# concatenate heads and put through final linear layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-84f09d35dbf9>\u001b[0m in \u001b[0;36mattention\u001b[0;34m(q, k, v, d_k, mask, dropout)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m   \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m  \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}